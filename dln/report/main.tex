\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{graphicx} 
\usepackage{minted}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}  
\usepackage{bm}
\usepackage[pdftex,bookmarks,colorlinks,breaklinks]{hyperref}  
\usepackage{memhfixc} 
\usepackage{pdfsync}  
\usepackage{fancyhdr}
\usepackage{subcaption}
\pagestyle{fancy}

\newcommand{\norm}[2][2]{\left\lVert #2 \right\rVert_{#1}}

\title{Training the Deep Linear Network}
\author{Dylan Hu}
\date{March 23, 2023}

\begin{document}
\maketitle
{
  \hypersetup{linkcolor=black}
  \tableofcontents
}
\section{Introduction}
With the expansion of computing capabilities in the past decade, deep approaches to machine learning formulated decades prior have finally become tractable. Moreover, with the abundance and accessibility of data as well as more complex and powerful model architectures, the impact and potential of deep learning and its applications have gained significant attention (pun intended).
\\\\
In just the past few months, accessible tools powered by deep learning such as ChatGPT have catapulted deep learning into the mainstream. While exciting new developments in deep learning seem to arise each week, we take a step back and analyze some of the fundamental mathematical principles behind deep learning.
\\\\
Arora, Cohen, and Hazan \cite{arora2018optimization} postulate that increased model \textit{depth} not only leads to greater expressiveness, but also that the \textit{overparameterization} leads to implicit acceleration of the optimization.
\\\\
In this report, we investigate and demonstrate the acceleration effect of overparameterization on the optimization of the deep linear network. We also explore further the mathematical basis behind overparameterization through empirical analysis.

\pagebreak

\section{Method}
We build the deep linear network (DLN) as formulated by Arora, Cohen, and Hazan \cite{arora2018optimization}. The DLN is a deep (multi-layer) neural network of $N$ linear layers of size $d \times d$ without activation functions. We therefore may represent the network as a chain of $N$ square $d \times d$ matrices:
$$\bm{W} = \left(W_N, W_{N-1}, \hdots, W_1\right)$$
The observable of the DLN is the product of the matrices in the chain:
$$W := \pi\left(\bm{W}\right) = W_N W_{N-1} \hdots W_1$$
We investigate the acceleration effect of overparameterization by comparing the optimization dynamics of the DLN to that of a single-layer linear network of size $d \times d$, where the chain $\bm{W}$ reduces to the observable $W$ and thus can be considered an end-to-end matrix $W$.

\subsection{Model}
We implement the DLN model in PyTorch:
\begin{minted}[fontsize=\footnotesize]{python}
class DLN(nn.Module):
    def __init__(self, d: int, N: int):
        super().__init__()
        self.d = d
        self.N = N
        self.params = nn.ParameterList(
            [nn.Parameter(torch.randn((d, d))) for _ in range(N)])

    def forward(self) -> Tensor:
        x = torch.eye(self.d)
        for param in self.params:
            x = param @ x
        return x
\end{minted}
\subsection{Optimization}
In investigating the effect of overparameterization on the optimization of the DLN, we consider the optimization problem of matrix completion. Given a random target $d \times d$ matrix $\Phi$, the optimization objective is to find $\bm{W}$ such that the observable $W$ has diagonal entries that match those of $\Phi$.
\\\\
We minimize the following energy functions:
$$E(\bm{W}) = \frac{1}{2} \norm{\text{diag}\left(\Phi - \pi\left(\bm{W}\right)\right)}^2,\quad F(W) = \frac{1}{2} \norm{\text{diag}\left(\Phi - W\right)}^2$$
Note that we distinguish $E\left(\bm{W}\right)$ from $F(W)$, as they have different domains and induce different gradient dynamics for optimization.
\\\\
We denote optimization on the chain of matrices $\bm{W}$ as operating on the ``upstairs'', whereas optimization on the end-to-end matrix $W$ is operating on the ``downstairs''.
\\\\
Training on the upstairs evolves each of the matrices of $\bm{W}$ according to the gradient flow
$$\dot{W}_i = -\nabla_{W_i}E\left(\bm{W}\right)$$
whereas training on the downstairs evolves the end-to-end matrix $W$ by the differential equation
\begin{equation} \label{eq:downstairs}
    \dot{W} = -\frac{1}{N}\sum_{j=1}^N \left(WW^T\right)^{\frac{N - j}{N}} \partial_WF\left(W\right)\left(W^TW\right)^\frac{j - 1}{N}
\end{equation}
where $\partial_WF\left(W\right) = -\text{diag}\left(\Phi - W\right)$.
\subsubsection{Loss function}
We train the DLN upstairs using supervision imposed by a mean-squared-error loss function for which minimization is equivalent to that for the energy function $E\left(\bm{W}\right)$:
$$\mathcal{L_\text{upstairs}} = \text{MSE}\left(\text{diag}\left(\Phi\right), \text{diag}\left(\pi\left(\bm{W}\right)\right)\right) = \frac{1}{d}\sum_{i=1}^d \text{diag}\left(\Phi - \pi\left(\bm{W}\right)\right)_i^2$$
Although this is not the loss function for the downstairs training, we return this loss for both upstairs and downstairs training for the sake of comparison:
$$\mathcal{L_\text{downstairs}} = \text{MSE}\left(\text{diag}\left(\Phi\right), \text{diag}\left(W\right)\right) = \frac{1}{d}\sum_{i=1}^d \text{diag}\left(\Phi - W\right)_i^2$$

\subsubsection{Training upstairs}
The implementation of the upstairs training uses PyTorch's automatic differentiation to compute the gradient of the loss function with respect to the parameters of the DLN. We then update the parameters of the DLN using a built-in optimizer. We experiment with a variety of optimizers, the results of which are discussed in Section \ref{sec:further}.
\begin{minted}[fontsize=\footnotesize]{python}
def upstairs_func(model: DLN,
                  target: Tensor,
                  optimizer: torch.optim.Optimizer,
                  scheduler: Optional[torch.optim.lr_scheduler.LRScheduler]
                    = None):
    def upstairs(train: bool) -> Tensor:
        model.train(train)
        with torch.set_grad_enabled(train):
            optimizer.zero_grad()
            W = model.forward()
            if train:
                loss = F.mse_loss(torch.diag(W), torch.diag(target))
                loss.backward()
                optimizer.step()
                if scheduler:
                    scheduler.step()
                return loss
        return W
    return upstairs

upstairs_model = DLN(d, N) # by default, d = 2, N = 3
optimizer = torch.optim.SGD(upstairs_model.parameters(), lr)
target = torch.randn((d, d))

iteration = 0
upstairs_losses = []

upstairs = upstairs_func(upstairs_model, target, optimizer)
while (loss := upstairs(train=True)) > 1e-7:
    upstairs_losses.append(loss.item())
    iteration += 1
\end{minted}

\subsubsection{Training downstairs}
The downstairs training does not use PyTorch's automatic differentiation but instead uses manual gradient computation and composition according to Eq. \ref {eq:downstairs}.
\begin{minted}[fontsize=\footnotesize]{python}
def downstairs_func(model: DLN, target: Tensor, lr: float, N: int):
    def downstairs() -> Tensor:
        model.train(False)
        with torch.no_grad():
            W = model.forward()
            d_F = torch.diag_embed(torch.diag(W - target))
            WWT = W @ W.T
            WTW = W.T @ W
            d_W = torch.zeros_like(W)
            for i in range(1, N + 1):
                d_W = d_W + \
                    scipy.linalg.fractional_matrix_power(
                        WWT, (N - i) / N) @ \
                    np.array(d_F) @ \
                    scipy.linalg.fractional_matrix_power(
                        WTW, (i - 1) / N)
            d_W = d_W * torch.ones_like(W) / N
            W = W - lr * d_W
            model.params[0] = W
            return F.mse_loss(torch.diag(W), torch.diag(target))
    return downstairs
\end{minted}

\subsection{Further exploration}

\subsection{Network width}
We experiment with $d=4, d=6$ network widths with a fixed $N = 3$. Results are shown in Figure \ref{fig:results:width}.

\subsubsection{Network depth}
We experiment with $N > 3$ network depths with a fixed $d = 2$. Results are shown in Figure \ref{fig:results:depth}.

\subsubsection{Optimizers}
For the results in Section \ref{sec:results:acceleration}, we use stochastic gradient descent (SGD) for upstairs. However, we also experiment with other optimizers, including Adam and RMSprop. For all tests, we use $d = 2, N = 3$. The results are shown in Figure \ref{fig:results:optimizers}.

\subsubsection{Learning rate}
We also experiment with different learning rates with SGD for both upstairs and downstairs training. As expected, we find that the learning rate has a significant effect on the performance of the DLN, with higher learning rates amenable to shallow networks but not deep networks, where smaller learning rates are required.
\\\\
This reflects the intuition that as the network becomes deeper, the manifold of possible solutions becomes more complex, and the DLN must be more careful in its exploration of the manifold.
\\\\
The results are shown in Figure \ref{fig:results:lr}.



\pagebreak

\section{Results}
\subsection{Implicit acceleration} \label{sec:results:acceleration}
\begin{figure}[ht]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_avg_lr1e-2.png}
        \caption{100 trial average}
        \label{fig:upstairs_avg}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_1_lr1e-2.png}
        \caption{a particular trajectory}
        \label{fig:upstairs_1}
    \end{subfigure}
    \caption{Training upstairs with $d = 2, N = 3$, and learning rate = 0.01}
    \label{fig:upstairs}
\end{figure}
\begin{figure}[ht]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/downstairs_avg_lr1e-2.png}
        \caption{100 trial average}
        \label{fig:downstairs_avg}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/downstairs_1_lr1e-2.png}
        \caption{a particular trajectory}
        \label{fig:downstairs_1}
    \end{subfigure}
    \caption{Training downstairs with $d = 2, N = 3$, and learning rate = 0.01}
    \label{fig:downstairs}
\end{figure}
\subsubsection{Training upstairs}
With $d = 2, N = 3$, and learning rate = 0.01, we are able to successfully train and consistently converge the DLN upstairs using the SGD optimizer. Measured across 100 trials, it takes an average of $204.58$ iterations with standard deviation 101.03 iterations to converge. The average trajectory and a particular trajectory are shown in Figure \ref{fig:upstairs}.

\subsubsection{Training downstairs}
With $d = 2, N = 3$, and learning rate = 0.01, we are also able to successfully train and consistently converge downstairs. However, we find that the convergence is much slower and the trajectory is much more varied than upstairs. Measured across 100 trials, it takes an average of 1410.79 iterations with standard deviation of 1019.89 iterations to converge. The average trajectory and a particular trajectory are shown in Figure \ref{fig:downstairs}.

\subsubsection{Comparison}
We observe that the convergence of the upstairs occurs more quickly and in a more predictable fashion than that of the downstairs, supporting the claim that overparameterization can lead to implicit acceleration. We plot average and particular trajectories for both upstairs and downstairs in Figure \ref{fig:upstairs_downstairs}.
\begin{figure}[ht]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_downstairs_avg_lr1e-2.png}
        \caption{100 trial average}
        \label{fig:upstairs_downstairs_avg}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_downstairs_1_lr1e-2.png}
        \caption{a particular trajectory}
        \label{fig:upstairs_downstairs_1}
    \end{subfigure}
    \caption{Training upstairs compared to downstairs with $d = 2, N = 3$, and learning rate = 0.01}
    \label{fig:upstairs_downstairs}
\end{figure}

\subsection{Further exploration}

\subsubsection{Network width}
\begin{figure}[h!]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_avg_d4.png}
        \caption{100 trial average}
        \label{fig:upstairs_avg_d4}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_avg_d6.png}
        \caption{a particular trajectory}
        \label{fig:upstairs_avg_d6}
    \end{subfigure}
    \caption{$d = 2$ and $d=6$ for $N = 3$ and learning rate = 0.01}
    \label{fig:results:width}
\end{figure}
Only $N=5$ resulted in reliable enough convergence across 100 trials; the results are shown in Figure \ref{fig:results:depth}. With $N > 5$, the DLN is able to converge, but the convergence becomes less and less reliable.
\\\\
We hypothesize that this is the vanishing or exploding gradient problem, which is a common problem in deep learning. The DLN is able to converge with $N=5$ because the gradient cannot compound to the point where it becomes too large or too small. We therefore experiment by adding various activations between each layer of the DLN.
\\\\
With ReLU activations, the model fails to converge at $N = 7$ and becomes stuck at a local minimum. With sigmoid activations, the model also fails to converge reliably but does consistently reach better local minima than ReLU. Lastly, with the tanh activation, the model does indeed reliably converge at $N = 7$, and it seems to reliably converge up until at least $N = 80$! Greater $N$ could not be tested for eventual convergence due to the increased runtime.
\\\\
We leave a principled explanation for the success of the tanh activation in this setting as future work.

\subsubsection{Network depth}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/upstairs_avg_n5.png}
    \caption{N = 5}
    \label{fig:results:depth}
\end{figure}
In the case of $d=4$, the mean number of iterations measured across 100 trials is 142.34 iterations with standard deviation 159.37. In the case of $d=6$, the mean number of iterations measured across 100 trials is 79.03 iterations with standard deviation 44.76. In general, we find that increasing network width results in faster convergence, as there are more parameters to simultaneously tune in order to reduce the mean error. The results are shown in Figure \ref{fig:results:depth}.

\subsubsection{Optimizers}
We find that RMSprop performs best and slightly better than SGD, while Adam performs worst. The results are shown in Figure \ref{fig:results:optimizers}.
\\\\
A potential intuition for these results is that RMSprop and SGD are both first-order methods, while Adam is a first-order and second-order method. As the gradient flow is a first-order differential equation, it is possible that first-order optimizers are better suited for this task.
\\\\
Another potential intuition is that Adam may simply be too agressive and tend to overshoot. In fact, we notice that trajectories with adam are significantly less smooth.
\\\\
A more principled investigation of the effects of different optimizers is left for future work.
\begin{figure}[h!]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_avg_optimizers.png}
        \caption{100 trial average}
        \label{fig:upstairs_avg_optimizers}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_1_optimizers.png}
        \caption{a particular trajectory}
        \label{fig:upstairs_1_optimizers}
    \end{subfigure}
    \caption{SGD, Adam, and RMSprop compared with $d = 2, N = 3$, and learning rate = 0.01}
    \label{fig:results:optimizers}
\end{figure}



\subsubsection{Learning rate}
With a learning rate of 0.05, the mean number of iterations for convergence is 51.67 with a standard deviation of 37.75. With a learning rate of 0.001, the mean number of iterations for convergence is 4811.72 with a standard deviation of 4742.84.
\begin{figure}[h!]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_avg_lr5e-2.png}
        \caption{Learning rate = 0.05}
        \label{fig:upstairs_avg_lr}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/upstairs_avg_lr1e-3.png}
        \caption{Learning rate = 0.001}
        \label{fig:upstairs_1_lr}
    \end{subfigure}
    \caption{SGD with learning rates 0.05 and 0.001 with $d = 2, N = 3$}
    \label{fig:results:lr}
\end{figure}
We find that a learning rate of 0.05 is too large for even $N = 5$ to converge, whereas a learning rate of 0.01 failed to converge five out of 100 trials, and a learning rate of 0.001 consistently enabled convergence.
\\\\
This matches intuition, as deeper networks induce a more complex loss landscape, and a larger learning rate can lead to a more unstable trajectory. We hypothesize that the fragility in these experiments is caused by the lack of activation functions which are essential for avoiding vanishing or exploding gradients. As such, the DLN efficiently demonstrates both the effect of learning rate on deep networks and the importance of activation functions.

\pagebreak

\bibliographystyle{plain}
\bibliography{main}

\end{document}